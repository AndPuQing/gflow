<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>GPU Management - gflow Documentation</title>


        <!-- Custom HTML head -->

        <meta name="description" content="A lightweight, single-node job scheduler inspired by Slurm">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">gflow Documentation</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="gpu-management"><a class="header" href="#gpu-management">GPU Management</a></h1>
<p>This guide covers how gflow manages GPU resources, from detection to allocation and monitoring.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>gflow provides automatic GPU detection, allocation, and management for NVIDIA GPUs through the NVML library. It ensures efficient GPU utilization across multiple jobs while preventing resource conflicts.</p>
<h2 id="gpu-detection"><a class="header" href="#gpu-detection">GPU Detection</a></h2>
<h3 id="checking-available-gpus"><a class="header" href="#checking-available-gpus">Checking Available GPUs</a></h3>
<p>View system GPU information:</p>
<pre><code class="language-bash">$ ginfo info
</code></pre>
<p><strong>Information displayed</strong>:</p>
<ul>
<li>Total number of GPUs in the system</li>
<li>Number of currently available (unused) GPUs</li>
<li>GPU model and UUID for each device</li>
<li>Current allocation status (available or in use by which job)</li>
</ul>
<h3 id="requirements"><a class="header" href="#requirements">Requirements</a></h3>
<p><strong>System requirements</strong>:</p>
<ul>
<li>NVIDIA GPU(s)</li>
<li>NVIDIA drivers installed</li>
<li>NVML library available (<code>libnvidia-ml.so</code>)</li>
</ul>
<p><strong>Verify GPU setup</strong>:</p>
<pre><code class="language-bash"># Check NVIDIA driver
nvidia-smi

# Check NVML library
ldconfig -p | grep libnvidia-ml

# Test GPU detection with gflow
gflowd up
ginfo info
</code></pre>
<h3 id="no-gpu-systems"><a class="header" href="#no-gpu-systems">No GPU Systems</a></h3>
<p>gflow works perfectly fine on systems without GPUs:</p>
<ul>
<li>GPU detection fails gracefully</li>
<li>All features work except GPU allocation</li>
<li>Jobs can still be submitted without <code>--gpus</code> flag</li>
</ul>
<h2 id="gpu-allocation"><a class="header" href="#gpu-allocation">GPU Allocation</a></h2>
<h3 id="requesting-gpus"><a class="header" href="#requesting-gpus">Requesting GPUs</a></h3>
<p>Request GPUs when submitting jobs:</p>
<pre><code class="language-bash"># Request 1 GPU
gbatch --gpus 1 python train.py

# Request 2 GPUs
gbatch --gpus 2 python multi_gpu_train.py

# Request 4 GPUs
gbatch --gpus 4 python distributed_train.py
</code></pre>
<h3 id="automatic-gpu-assignment"><a class="header" href="#automatic-gpu-assignment">Automatic GPU Assignment</a></h3>
<p>When a job requests GPUs:</p>
<ol>
<li>Scheduler checks for available GPUs</li>
<li>Assigns specific GPU IDs to the job</li>
<li>Sets <code>CUDA_VISIBLE_DEVICES</code> environment variable</li>
<li>Job sees only its allocated GPUs (numbered 0, 1, 2, ...)</li>
</ol>
<p><strong>Example</strong>:</p>
<pre><code class="language-bash"># Submit job requesting 2 GPUs
$ gbatch --gpus 2 nvidia-smi

# Check allocation
$ gqueue -f JOBID,NAME,NODES,NODELIST
JOBID    NAME                NODES    NODELIST(REASON)
42       brave-river-1234    2        1,2

# Inside the job, CUDA_VISIBLE_DEVICES=1,2
# But CUDA will renumber them as 0,1 for the application
</code></pre>
<h3 id="gpu-visibility"><a class="header" href="#gpu-visibility">GPU Visibility</a></h3>
<p>gflow uses <code>CUDA_VISIBLE_DEVICES</code> to control GPU access:</p>
<pre><code class="language-python"># In your job (Python example)
import os
import torch

# gflow sets this automatically
print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")

# CUDA sees only allocated GPUs
print(f"Visible GPUs to CUDA: {torch.cuda.device_count()}")

# Use GPUs normally (indexed from 0)
device = torch.device('cuda:0')  # First allocated GPU
</code></pre>
<p><strong>Bash example</strong>:</p>
<pre><code class="language-bash">#!/bin/bash
# GFLOW --gpus 2

echo "Allocated GPUs: $CUDA_VISIBLE_DEVICES"
nvidia-smi --query-gpu=index,name,memory.free --format=csv
python train.py
</code></pre>
<h2 id="gpu-scheduling"><a class="header" href="#gpu-scheduling">GPU Scheduling</a></h2>
<h3 id="job-queue-with-gpu-requests"><a class="header" href="#job-queue-with-gpu-requests">Job Queue with GPU Requests</a></h3>
<p>Jobs wait for GPUs when none are available:</p>
<pre><code class="language-bash"># System has 2 GPUs

# Job 1: Uses 2 GPUs
$ gbatch --gpus 2 python long_train.py
Submitted batch job 1

# Job 2: Requests 1 GPU (must wait)
$ gbatch --gpus 1 python train.py
Submitted batch job 2

$ gqueue
JOBID    NAME      ST    NODES    NODELIST(REASON)
1        job-1     R     2        0,1
2        job-2     PD    1        (Resources)
</code></pre>
<p>Job 2 waits until Job 1 releases at least 1 GPU.</p>
<h3 id="priority-and-gpu-allocation"><a class="header" href="#priority-and-gpu-allocation">Priority and GPU Allocation</a></h3>
<p>Higher priority jobs get GPUs first:</p>
<pre><code class="language-bash"># Low priority job
gbatch --priority 5 --gpus 1 python task1.py

# High priority job
gbatch --priority 100 --gpus 1 python urgent_task.py
</code></pre>
<p>When GPUs become available:</p>
<ol>
<li>Scheduler selects highest priority queued job</li>
<li>Checks if enough GPUs are free</li>
<li>Allocates GPUs and starts the job</li>
</ol>
<h3 id="partial-gpu-availability"><a class="header" href="#partial-gpu-availability">Partial GPU Availability</a></h3>
<p>If a job requests more GPUs than currently available, it waits:</p>
<pre><code class="language-bash"># System has 4 GPUs, 3 in use

# This waits for 4 GPUs
gbatch --gpus 4 python distributed_train.py

$ gqueue
JOBID    NAME      ST    NODES    NODELIST(REASON)
5        job-5     PD    4        (Resources: Need 4 GPUs, only 1 available)
</code></pre>
<h2 id="monitoring-gpu-usage"><a class="header" href="#monitoring-gpu-usage">Monitoring GPU Usage</a></h2>
<h3 id="check-current-gpu-allocation"><a class="header" href="#check-current-gpu-allocation">Check Current GPU Allocation</a></h3>
<pre><code class="language-bash"># View GPU allocation for running jobs
$ gqueue -s Running -f JOBID,NAME,NODES,NODELIST
JOBID    NAME                NODES    NODELIST(REASON)
1        train-resnet        1        0
2        train-vit           1        1
3        train-bert          2        2,3
</code></pre>
<h3 id="system-wide-gpu-status"><a class="header" href="#system-wide-gpu-status">System-wide GPU Status</a></h3>
<pre><code class="language-bash"># View system info
$ ginfo info

# Use nvidia-smi for real-time monitoring
watch -n 1 nvidia-smi
</code></pre>
<h3 id="per-job-gpu-usage"><a class="header" href="#per-job-gpu-usage">Per-job GPU Usage</a></h3>
<pre><code class="language-bash"># Submit job with GPU monitoring
cat &gt; monitor_gpu.sh &lt;&lt; 'EOF'
#!/bin/bash
# GFLOW --gpus 1

echo "=== GPU Allocation ==="
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

echo "=== GPU Details ==="
nvidia-smi --query-gpu=index,name,memory.total,memory.free,utilization.gpu \
           --format=csv

echo "=== Training ==="
python train.py
EOF

chmod +x monitor_gpu.sh
gbatch monitor_gpu.sh
</code></pre>
<p>Check the log:</p>
<pre><code class="language-bash">cat ~/.local/share/gflow/logs/&lt;job_id&gt;.log
</code></pre>
<h2 id="multi-gpu-training"><a class="header" href="#multi-gpu-training">Multi-GPU Training</a></h2>
<h3 id="data-parallel-training-pytorch"><a class="header" href="#data-parallel-training-pytorch">Data Parallel Training (PyTorch)</a></h3>
<pre><code class="language-python"># train.py
import torch
import torch.nn as nn

# gflow sets CUDA_VISIBLE_DEVICES automatically
device_count = torch.cuda.device_count()
print(f"Using {device_count} GPUs")

model = MyModel()
if device_count &gt; 1:
    model = nn.DataParallel(model)
model = model.cuda()

# Train normally
train(model)
</code></pre>
<p>Submit with multiple GPUs:</p>
<pre><code class="language-bash">gbatch --gpus 2 python train.py
</code></pre>
<h3 id="distributed-training-pytorch"><a class="header" href="#distributed-training-pytorch">Distributed Training (PyTorch)</a></h3>
<pre><code class="language-python"># distributed_train.py
import torch
import torch.distributed as dist

def main():
    # gflow allocates GPUs via CUDA_VISIBLE_DEVICES
    world_size = torch.cuda.device_count()

    # Initialize process group
    dist.init_process_group(backend='nccl', world_size=world_size)

    # Get local rank
    local_rank = dist.get_rank()
    torch.cuda.set_device(local_rank)

    # Training code
    train(local_rank)

if __name__ == '__main__':
    main()
</code></pre>
<p>Submit:</p>
<pre><code class="language-bash">gbatch --gpus 4 python distributed_train.py
</code></pre>
<h3 id="tensorflow-multi-gpu"><a class="header" href="#tensorflow-multi-gpu">TensorFlow Multi-GPU</a></h3>
<pre><code class="language-python"># tf_train.py
import tensorflow as tf

# Let TensorFlow see all allocated GPUs
gpus = tf.config.list_physical_devices('GPU')
print(f"Available GPUs: {len(gpus)}")

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    model = create_model()
    model.compile(...)

model.fit(...)
</code></pre>
<p>Submit:</p>
<pre><code class="language-bash">gbatch --gpus 2 python tf_train.py
</code></pre>
<h2 id="advanced-gpu-management"><a class="header" href="#advanced-gpu-management">Advanced GPU Management</a></h2>
<h3 id="gpu-memory-considerations"><a class="header" href="#gpu-memory-considerations">GPU Memory Considerations</a></h3>
<p>Even if GPUs are "available", they might have insufficient memory:</p>
<pre><code class="language-bash"># Check GPU memory before submitting large jobs
nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits

# Example: Job needs 20GB per GPU
gbatch --gpus 1 python memory_intensive_train.py
</code></pre>
<p><strong>Note</strong>: gflow tracks GPU allocation, not memory usage. Plan accordingly.</p>
<h3 id="exclusive-gpu-access"><a class="header" href="#exclusive-gpu-access">Exclusive GPU Access</a></h3>
<p>Each job gets exclusive access to its allocated GPUs:</p>
<ul>
<li>No other gflow job can use them</li>
<li>Other processes (outside gflow) can still access them</li>
<li>Use <code>CUDA_VISIBLE_DEVICES</code> to ensure isolation</li>
</ul>
<h3 id="mixed-gpucpu-jobs"><a class="header" href="#mixed-gpucpu-jobs">Mixed GPU/CPU Jobs</a></h3>
<p>Run CPU and GPU jobs simultaneously:</p>
<pre><code class="language-bash"># CPU-only job
gbatch python cpu_task.py

# GPU job
gbatch --gpus 1 python gpu_task.py
</code></pre>
<p>CPU jobs don't consume GPU slots and can run in parallel with GPU jobs.</p>
<h2 id="gpu-job-patterns"><a class="header" href="#gpu-job-patterns">GPU Job Patterns</a></h2>
<h3 id="sequential-gpu-pipeline"><a class="header" href="#sequential-gpu-pipeline">Sequential GPU Pipeline</a></h3>
<p>Release GPUs between stages:</p>
<pre><code class="language-bash"># Stage 1: Preprocessing (no GPU)
ID1=$(gbatch --time 30 python preprocess.py | grep -oP '\d+')

# Stage 2: Training (uses GPU)
ID2=$(gbatch --depends-on $ID1 --gpus 1 --time 4:00:00 \
             python train.py | grep -oP '\d+')

# Stage 3: Evaluation (no GPU)
gbatch --depends-on $ID2 --time 10 python evaluate.py
</code></pre>
<p><strong>Benefit</strong>: GPU is free during preprocessing and evaluation.</p>
<h3 id="parallel-multi-gpu-experiments"><a class="header" href="#parallel-multi-gpu-experiments">Parallel Multi-GPU Experiments</a></h3>
<p>Run experiments in parallel on different GPUs:</p>
<pre><code class="language-bash"># Each gets one GPU
gbatch --gpus 1 --time 2:00:00 --config config1.yaml --name "exp1" python train.py
gbatch --gpus 1 --time 2:00:00 --config config2.yaml --name "exp2" python train.py
gbatch --gpus 1 --time 2:00:00 --config config3.yaml --name "exp3" python train.py
</code></pre>
<p>If you have 4 GPUs, the first 4 jobs run in parallel.</p>
<h3 id="dynamic-gpu-scaling"><a class="header" href="#dynamic-gpu-scaling">Dynamic GPU Scaling</a></h3>
<p>Start with fewer GPUs, scale up later:</p>
<pre><code class="language-bash"># Initial experiment (1 GPU)
gbatch --gpus 1 --time 1:00:00 python train.py --test-run

# Full training (4 GPUs) - submit after validation
gbatch --gpus 4 --time 8:00:00 python train.py --full
</code></pre>
<h3 id="hyperparameter-sweep-with-gpus"><a class="header" href="#hyperparameter-sweep-with-gpus">Hyperparameter Sweep with GPUs</a></h3>
<pre><code class="language-bash"># Grid search across 4 GPUs
for lr in 0.001 0.01 0.1; do
    for batch_size in 32 64 128; do
        gbatch --gpus 1 --time 3:00:00 \
               --name "lr${lr}_bs${batch_size}" \
               python train.py --lr $lr --batch-size $batch_size
    done
done

# Monitor GPU allocation
watch -n 2 'gqueue -s Running,Queued -f JOBID,NAME,NODES,NODELIST'
</code></pre>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="issue-job-not-getting-gpu"><a class="header" href="#issue-job-not-getting-gpu">Issue: Job not getting GPU</a></h3>
<p><strong>Possible causes</strong>:</p>
<ol>
<li>
<p><strong>Forgot to request GPU</strong>:</p>
<pre><code class="language-bash"># Wrong - no GPU requested
gbatch python train.py

# Correct
gbatch --gpus 1 python train.py
</code></pre>
</li>
<li>
<p><strong>All GPUs in use</strong>:</p>
<pre><code class="language-bash"># Check allocation
gqueue -s Running -f NODES,NODELIST
ginfo info
</code></pre>
</li>
<li>
<p><strong>Job is queued</strong>:</p>
<pre><code class="language-bash"># Job waits for GPU
$ gqueue -j &lt;job_id&gt; -f JOBID,ST,NODES,NODELIST
JOBID    ST    NODES    NODELIST(REASON)
42       PD    1        (Resources)
</code></pre>
</li>
</ol>
<h3 id="issue-job-sees-wrong-gpus"><a class="header" href="#issue-job-sees-wrong-gpus">Issue: Job sees wrong GPUs</a></h3>
<p><strong>Check CUDA_VISIBLE_DEVICES</strong>:</p>
<pre><code class="language-bash"># In your job script
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

# Should match gqueue output
gqueue -f JOBID,NODELIST
</code></pre>
<h3 id="issue-out-of-memory-error"><a class="header" href="#issue-out-of-memory-error">Issue: Out of memory error</a></h3>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Request more GPUs: <code>--gpus 2</code></li>
<li>Reduce batch size in your code</li>
<li>Use gradient accumulation</li>
<li>Enable mixed precision training (FP16)</li>
</ol>
<p><strong>Check memory</strong>:</p>
<pre><code class="language-bash">nvidia-smi --query-gpu=memory.free,memory.used --format=csv
</code></pre>
<h3 id="issue-gpu-utilization-low"><a class="header" href="#issue-gpu-utilization-low">Issue: GPU utilization low</a></h3>
<p><strong>Possible causes</strong>:</p>
<ul>
<li>Data loading bottleneck (use more workers)</li>
<li>CPU preprocessing bottleneck</li>
<li>Small batch size</li>
<li>Model too small for GPU</li>
</ul>
<p><strong>Debug</strong>:</p>
<pre><code class="language-bash"># Monitor GPU utilization
watch -n 1 nvidia-smi

# Check job logs for bottlenecks
tail -f ~/.local/share/gflow/logs/&lt;job_id&gt;.log
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<ol>
<li><strong>Request only needed GPUs</strong>: Don't over-allocate resources</li>
<li><strong>Monitor GPU usage</strong>: Use <code>nvidia-smi</code> to verify utilization</li>
<li><strong>Optimize data loading</strong>: Prevent GPU starvation</li>
<li><strong>Use mixed precision</strong>: Reduce memory usage with FP16</li>
<li><strong>Batch jobs efficiently</strong>: Group similar GPU requirements</li>
<li><strong>Release GPUs early</strong>: Use dependencies to chain CPU/GPU stages</li>
<li><strong>Test on 1 GPU first</strong>: Validate before scaling to multiple GPUs</li>
<li><strong>Set time limits</strong>: Prevent GPU hogging by runaway jobs</li>
<li><strong>Log GPU stats</strong>: Include GPU info in job logs</li>
<li><strong>Clean up checkpoints</strong>: Manage disk space when using GPUs</li>
</ol>
<h2 id="performance-tips"><a class="header" href="#performance-tips">Performance Tips</a></h2>
<h3 id="maximize-gpu-utilization"><a class="header" href="#maximize-gpu-utilization">Maximize GPU Utilization</a></h3>
<pre><code class="language-python"># Increase batch size
train_loader = DataLoader(dataset, batch_size=128, num_workers=8)

# Use pin_memory for faster transfers
train_loader = DataLoader(dataset, batch_size=128, pin_memory=True)

# Enable AMP for mixed precision
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    output = model(input)
    loss = criterion(output, target)
</code></pre>
<h3 id="efficient-multi-gpu-usage"><a class="header" href="#efficient-multi-gpu-usage">Efficient Multi-GPU Usage</a></h3>
<pre><code class="language-python"># Use DistributedDataParallel instead of DataParallel
from torch.nn.parallel import DistributedDataParallel as DDP

# More efficient communication
model = DDP(model, device_ids=[local_rank])
</code></pre>
<h3 id="monitor-and-optimize"><a class="header" href="#monitor-and-optimize">Monitor and Optimize</a></h3>
<pre><code class="language-bash">#!/bin/bash
# GFLOW --gpus 1

# Log GPU stats before training
nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv -l 10 &gt; gpu_stats.log &amp;
GPU_MONITOR_PID=$!

# Run training
python train.py

# Stop monitoring
kill $GPU_MONITOR_PID
</code></pre>
<h2 id="reference"><a class="header" href="#reference">Reference</a></h2>
<h3 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Set By</th><th>Description</th></tr></thead><tbody>
<tr><td><code>CUDA_VISIBLE_DEVICES</code></td><td>gflow</td><td>Comma-separated GPU IDs (e.g., "0,1")</td></tr>
</tbody></table>
</div>
<h3 id="gpu-related-commands"><a class="header" href="#gpu-related-commands">GPU-Related Commands</a></h3>
<pre><code class="language-bash"># Check system GPUs
ginfo info

# Submit job with GPUs
gbatch --gpus &lt;N&gt; ...

# Check GPU allocation
gqueue -f JOBID,NODES,NODELIST

# Monitor running GPU jobs
gqueue -s Running -f JOBID,NODES,NODELIST

# Monitor system GPUs
nvidia-smi
watch -n 1 nvidia-smi
</code></pre>
<h2 id="see-also"><a class="header" href="#see-also">See Also</a></h2>
<ul>
<li><a href="./job-submission.html">Job Submission</a> - Complete job submission guide</li>
<li><a href="./job-dependencies.html">Job Dependencies</a> - Workflow management</li>
<li><a href="./time-limits.html">Time Limits</a> - Job timeout management</li>
<li><a href="../reference/quick-reference.html">Quick Reference</a> - Command cheat sheet</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../user-guide/job-dependencies.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../user-guide/time-limits.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../user-guide/job-dependencies.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../user-guide/time-limits.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->



    </div>
    </body>
</html>
